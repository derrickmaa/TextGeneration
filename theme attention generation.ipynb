{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data should be csv.\n",
    "with 4 col ['theme', 'keyword', 'src', 'tgt']\n",
    "length of 'theme' must be 1\n",
    "\"\"\"\n",
    "device = torch.device('cuda:0')\n",
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 10\n",
    "HIDDEN_SIZE = 256\n",
    "N_EPOCHS = 51\n",
    "CLIP = 1\n",
    "LEARNING_RATE = 3e-5 \n",
    "ENC_DROPOUT = 0.2 # not use\n",
    "DEC_DROPOUT = 0.2 # not use\n",
    "TEACH_RATE = 1\n",
    "LAMBDA_COVERAGE = 1\n",
    "THEME_LEN = 1 # fixed set as 1\n",
    "KEYWORD_LEN = 1 # fixed set as 1\n",
    "SRC_LEN = 80\n",
    "TGT_LEN = 80\n",
    "VOCAB_MIN_FREQUENCY = 1\n",
    "GENERATION_LEN = 80\n",
    "CHECKPOINT_TIMES = 8 # every XX times save the model and parameters\n",
    "TRAINING_DATA_PATH = '...'\n",
    "TEST_DATA_PATH = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    \n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.wordfrequency = {}\n",
    "        self.word2index = {'<unk>': 0, '<pad>': 1, '<sos>': 2, '<eos>': 3}\n",
    "        self.word2count = {'<unk>': 0, '<pad>': 0, '<sos>': 0, '<eos>': 0}\n",
    "        self.index2word = {0: '<unk>', 1: \"<pad>\", 2: \"<sos>\", 3:'<eos>'}\n",
    "        self.n_words = 4  # Count SOS and EOS and PAD and UNK\n",
    "\n",
    "    def get_vocab(self, n): \n",
    "        for word in self.wordfrequency.keys():\n",
    "            self.add_word(word, n)\n",
    "            \n",
    "    def get_frequency(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.frequency(word)\n",
    "            \n",
    "    def add_word(self, word, n):\n",
    "        if self.wordfrequency[word] >= n:\n",
    "            if word not in self.word2index: \n",
    "                self.word2index[word] = self.n_words\n",
    "                self.word2count[word] = self.wordfrequency[word]\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            else:\n",
    "                self.word2count[word] = self.wordfrequency[word]\n",
    "        else:\n",
    "            self.word2count['<unk>'] += 1\n",
    "            \n",
    "    def frequency(self, word):\n",
    "        if word not in self.wordfrequency:\n",
    "            self.wordfrequency[word] = 1\n",
    "        else:\n",
    "            self.wordfrequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATA_PATH, 'r', encoding='utf-8-sig') as r:\n",
    "    r1 = r.readlines()\n",
    "    r1 = r1[1:]\n",
    "    lang = Lang(r1)\n",
    "    for i in range(len(r1)):\n",
    "        j = r1[i].split(',')\n",
    "        b1 = j[0].strip(' ').strip('\\n').strip(' ')\n",
    "        b2 = j[1].strip(' ').strip('\\n').strip(' ')\n",
    "        b3 = j[2].strip(' ').strip('\\n').strip(' ')\n",
    "        b4 = j[3].strip(' ').strip('\\n').strip(' ')\n",
    "        d = f'{b1} {b2} {b3} {b4}'\n",
    "        lang.get_frequency(d)\n",
    "    lang.get_vocab(VOCAB_MIN_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence, n):\n",
    "    ll = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word in lang.word2index:\n",
    "            ll.append(lang.word2index[word])\n",
    "        else:\n",
    "            ll.append(lang.word2index['<unk>'])\n",
    "            \n",
    "    while len(ll) < n:\n",
    "        ll.append(lang.word2index['<pad>'])      \n",
    "    if len(ll) > n:\n",
    "        ll = ll[:n-1]\n",
    "        ll.append(lang.word2index['<eos>'])        \n",
    "    return ll\n",
    "\n",
    "def tensor_from_sentence(lang, sentence, n):\n",
    "    indexes = indexes_from_sentence(lang, sentence, n)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
    "\n",
    "def tensors_from_pair(pair,  theme_len, keyword_len, src_len, tgt_len):\n",
    "    theme = tensor_from_sentence(lang, pair[0], theme_len)\n",
    "    keyword = tensor_from_sentence(lang, pair[1], keyword_len)\n",
    "    src = tensor_from_sentence(lang, pair[2], src_len)\n",
    "    tgt = tensor_from_sentence(lang, pair[3], tgt_len)\n",
    "    return (theme, keyword, src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datas(data_path, theme_len=THEME_LEN, keyword_len=KEYWORD_LEN, src_len=SRC_LEN, tgt_len=TGT_LEN):\n",
    "    datas = []\n",
    "    with open(data_path, 'r', encoding='utf-8-sig') as r:\n",
    "        r1 = r.readlines()\n",
    "        for i in range(1, len(r1)):\n",
    "            j = r1[i].split(',')\n",
    "            b1 = j[0].strip(' ').strip('\\n').strip(' ')\n",
    "            b2 = j[1].strip(' ').strip('\\n').strip(' ')\n",
    "            b3 = j[2].strip(' ').strip('\\n').strip(' ')\n",
    "            b4 = j[3].strip(' ').strip('\\n').strip(' ')\n",
    "            theme, keyword, src, tgt = tensors_from_pair([b1, b2, b3, b4], theme_len, keyword_len, src_len, tgt_len)\n",
    "            datas.append((theme, keyword, src, tgt))\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Dataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.datas = data\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.datas[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = get_datas(TRAINING_DATA_PATH)\n",
    "train_dataset = Create_Dataset(train_datas)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "DATA_SIZE = len(lang.index2word)\n",
    "print(DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.theme_embedding = nn.Embedding(data_size, hidden_size)\n",
    "        self.keyword_embedding = nn.Embedding(data_size, hidden_size)\n",
    "        self.src_embedding = nn.Embedding(data_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "        self.enc_hid_out = nn.Linear(hidden_size*2, hidden_size, bias=False)\n",
    "        self.enc_a_weight = nn.Linear(hidden_size*2, hidden_size, bias=True)\n",
    "        self.enc_theme_weight = nn.Linear(hidden_size, hidden_size)\n",
    "        self.enc_keyword_weight = nn.Linear(hidden_size, hidden_size)\n",
    "        self.enc_weight = nn.Linear(hidden_size, 1)\n",
    "        self.enc_out_out = nn.Linear(hidden_size*3, hidden_size, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def point(self, enc_theme, enc_keyword, enc_a): \n",
    "        ps_l = enc_a.size(1)\n",
    "        batch_size, pt_l, _ = enc_keyword.size()\n",
    "        b1 = self.enc_a_weight(enc_a.contiguous().view(-1, self.hidden_size*2)) # batch*src_len, hidden\n",
    "        b1 = b1.view(batch_size, ps_l, 1, self.hidden_size) # batch, src_len, 1, hidden\n",
    "        b1 = b1.expand(batch_size, ps_l, pt_l, self.hidden_size) # batch, src_len, theme_len, hidden\n",
    "        \n",
    "        \n",
    "        b22 = self.enc_theme_weight(enc_theme.contiguous().view(-1, self.hidden_size)) # batch*theme_len, hidden \n",
    "        b2 = b22.view(batch_size, 1, 1, self.hidden_size) # batch, 1, 1, hidden\n",
    "        b2 = b2.expand(batch_size, ps_l, pt_l, self.hidden_size) # batch, src_len, keyword_len, hidden\n",
    "        \n",
    "        b3 = self.enc_keyword_weight(enc_keyword.contiguous().view(-1, self.hidden_size)) # batch*keyword_len, hidden\n",
    "        b3 = b3.view(batch_size, 1, pt_l, self.hidden_size) # batch, 1, keyword_len, hidden\n",
    "        b3 = b3.expand(batch_size, ps_l, pt_l, self.hidden_size) # batch, src_len, keyword_len, hidden\n",
    "        \n",
    "        b = torch.tanh(b1 + b2 + b3) # batch, src_len, keyword_len, hidden_size (keyword_len=theme_len=1)!!!!!!\n",
    "\n",
    "        enc_w_0 = self.enc_weight(b.view(-1, self.hidden_size)).view(batch_size, ps_l, pt_l) # batch, src_len, keyword_len\n",
    "        \n",
    "        return torch.bmm(enc_w_0, b22.view(batch_size, 1, self.hidden_size).expand(batch_size, pt_l, self.hidden_size)) # batch, src_len, hidden\n",
    "    \n",
    "    def forward(self, theme, keyword, src):\n",
    "        batch_size = src.size(1)\n",
    "        src = src.view(-1, batch_size) # src_len, batch\n",
    "        theme = theme.view(-1, batch_size) # theme_len, batch\n",
    "        keyword = keyword.view(-1, batch_size) # keyword_len, batch\n",
    "        s_l = src.size(0)\n",
    "        t_l = theme.size(0)\n",
    "        src_emb = self.src_embedding(src) # src_len, batch, hidden\n",
    "        theme_emb = self.theme_embedding(theme) # theme_len, batch, hidden\n",
    "        keyword_emb = self.keyword_embedding(keyword) # keyword_len, batch, hidden\n",
    "        enc_output, enc_hidden = self.rnn(src_emb) # src_len, batch, hidden*2; 2, batch, hidden\n",
    "        enc_hidden = torch.cat([enc_hidden[0:enc_hidden.size(0):2],\n",
    "                                enc_hidden[1:enc_hidden.size(0):2]], 2) # 1, batch, hidden*2\n",
    "        enc_a = torch.cat([enc_output[:,:,:self.hidden_size],\n",
    "                          enc_output[:,:,self.hidden_size:]], 2) # src_len, batch, hidden*2\n",
    "        enc_theme = theme_emb.transpose(0, 1) # batch, theme_len, hidden\n",
    "        enc_a = enc_a.transpose(0, 1) # batch, src_len, hidden*2\n",
    "        enc_keyword = keyword_emb.transpose(0, 1) # batch, keyword_len, hidden\n",
    "        enc_w_2 = self.point(enc_theme, enc_keyword, enc_a) # batch, src_len, hidden\n",
    "\n",
    "        concat_enc_w = torch.cat([enc_w_2, enc_a], 2).view(batch_size*s_l, self.hidden_size*3) # batch*src_len, hidden*3\n",
    "        enc_out = self.enc_out_out(concat_enc_w).view(batch_size, s_l, self.hidden_size) # batch, src_len, hidden\n",
    "        enc_out = enc_out.transpose(0, 1).contiguous() # src_len, batch, hidden\n",
    "        \n",
    "        enc_hid = self.enc_hid_out(enc_hidden) # 1, batch, hidden\n",
    "        #enc_out = self.dropout(dec_out)\n",
    "        \n",
    "        return enc_out, enc_hid # src_len, batch, hidden; 1, batch, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_query = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.linear_context = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.linear_out = nn.Linear(hidden_size*2, hidden_size, bias=True)\n",
    "        self.linear_cover = nn.Linear(1, hidden_size, bias=False)\n",
    "        \n",
    "    def score(self, a_d_o, a_e_o):\n",
    "        batch_size, tgt_len, _ = a_d_o.size()\n",
    "        src_len = a_e_o.size(1)\n",
    "        a1 = self.linear_query(a_d_o.view(-1, self.hidden_size)) # batch*tgt_len, hidden\n",
    "        a1 = a1.view(batch_size, tgt_len, 1, self.hidden_size) # batch, tgt_len, 1, hidden\n",
    "        a1 = a1.expand(batch_size, tgt_len, src_len, self.hidden_size) # batch, tgt_len, src_len, hidden\n",
    "        \n",
    "        a2 = self.linear_context(a_e_o.contiguous().view(-1, self.hidden_size)) # batch*src_len, hidden \n",
    "        a2 = a2.view(batch_size, 1, src_len, self.hidden_size)\n",
    "        a2 = a2.expand(batch_size, tgt_len, src_len, self.hidden_size) # batch, tgt_len, src_len, hidden\n",
    "        \n",
    "        a = torch.tanh(a1 + a2) # batch, tgt_len, src_len, hidden_size\n",
    "\n",
    "        return self.v(a.view(-1, self.hidden_size)).view(batch_size, tgt_len, src_len) # batch, tgt_len, src_len\n",
    "    \n",
    "    def forward(self, attn_dec_state, attn_enc_state, attn_coverage):\n",
    "        d_o = attn_dec_state.permute(1, 0, 2) # batch, tgt_len, hidden\n",
    "        e_o = attn_enc_state.permute(1, 0, 2) # batch, src_len, hidden\n",
    "        batch_size, target_l, _= d_o.size()\n",
    "        source_l = e_o.size(1)\n",
    "        \n",
    "        if attn_coverage is not None:\n",
    "            cover = attn_coverage.view(-1).unsqueeze(1) # tgt_len*batch*src_len, 1(tgt_len=1)\n",
    "            a_o = self.linear_cover(cover).view(batch_size, source_l, self.hidden_size) # batch, src_len, hidden\n",
    "            e_o = e_o + a_o # batch, src_len, hidden\n",
    "            e_o = torch.tanh(e_o) # batch, src_len, hidden \n",
    "        \n",
    "        align = self.score(d_o.contiguous(), e_o.contiguous()) # batch, tgt_len, src_len\n",
    "        align_vectors = F.softmax(align.view(batch_size*target_l, source_l), -1) # batch*tgt_len, src_len\n",
    "        align_vectors = align_vectors.view(batch_size, target_l, source_l) # batch, tgt_len, src_len\n",
    "        c = torch.bmm(align_vectors, e_o)# batch, tgt_len, hidden\n",
    "\n",
    "        concat_c = torch.cat([c, d_o], 2).view(batch_size*target_l, self.hidden_size*2) # batch, tgt_len, hidden*2\n",
    "        attn_h = self.linear_out(concat_c).view(batch_size, target_l, self.hidden_size) # batch, tgt_len, hidden\n",
    "        attn_h2 = attn_h.permute(1, 0, 2).contiguous() # tgt_len, batch, hidden\n",
    "        align_vectors2 = align_vectors.permute(1, 0, 2).contiguous() # tgt_len, batch, src_len\n",
    "        \n",
    "        return attn_h2, align_vectors2 # tgt_len, batch, hidden; tgt_len, batch, src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, hidden_size, data_size, attention, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(data_size, hidden_size)\n",
    "        self.GRU = nn.GRU(hidden_size, hidden_size)\n",
    "        self.attn = attention\n",
    "        self.dec_out = nn.Linear(hidden_size, data_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, dec_tgt, dec_hidden, attn_memory, dec_g_t, teach, coverage, dec_coverage=True): # dec_g_t is ground truth\n",
    "        \n",
    "        if dec_g_t == None:\n",
    "            dec_emb = self.embedding(dec_tgt) # 1, batch, hidden\n",
    "        else:\n",
    "            if random.random() <= teach:\n",
    "                dec_emb = self.embedding(dec_g_t) # 1, batch, hidden\n",
    "            else:\n",
    "                dec_emb = self.embedding(dec_tgt) # 1, batch, hidden\n",
    "        dec_output, dec_hidden = self.GRU(dec_emb, dec_hidden) # tgt_len, batch, hidden; 1, batch, hidden\n",
    "        dec_output_attn, dec_cov = self.attn(dec_output, attn_memory, coverage) # tgt_len, batch, hidden; tgt_len, batch, src_len\n",
    "        if dec_coverage:\n",
    "            coverage = dec_cov if coverage is None else dec_cov + coverage # tgt_len, batch, src_len\n",
    "        dec_out = self.dec_out(dec_output_attn.contiguous().view(-1, self.hidden_size)) # tgt_len, batch, output\n",
    "        #dec_out = self.dropout(dec_out)\n",
    "        \n",
    "        return dec_out, dec_hidden, dec_cov, coverage # tgt_len, batch, output; 1, batch, hidden; tgt_len, batch, src_len; tgt_len, batch, src_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2S(nn.Module):\n",
    "    def __init__(self, encoder, decoder, output_size, gen_len):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.gen_len = gen_len-1\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_size = output_size\n",
    "                \n",
    "    def forward(self, theme, keyword, src, tgt, teach):\n",
    "        attns = {}\n",
    "        attns[\"std\"] = []\n",
    "        attns[\"coverage\"] = []\n",
    "        tgt_len, batch_size = tgt.size()\n",
    "        encoder_output, encoder_hidden = self.encoder(theme,\n",
    "                                                      keyword, src) # src_len, batch, hidden; 1, batch, hidden\n",
    "        decoder_hidden = encoder_hidden.view(1, batch_size, -1) # 1, batch, hidden\n",
    "        decoder_outputs = torch.ones(self.gen_len, batch_size, self.output_size, device=device) # tgt_len-1, batch\n",
    "        tgt_start = torch.full((1, batch_size), 2, dtype=torch.long, device=device) # 1, batch\n",
    "        decoder_attn_coverage = None\n",
    "        for i in range(tgt_len-1):\n",
    "            if i == 0:\n",
    "                current_tgt = tgt_start # 1, batch\n",
    "                g_t = None\n",
    "            elif i > 0: #and random.random() <= 0.7:\n",
    "                g_t = tgt[i].view(1, batch_size) # 1, batch\n",
    "            decoder_output, decoder_hidden, decoder_attn_std, decoder_attn_coverage = self.decoder(current_tgt, decoder_hidden, encoder_output, g_t, teach, decoder_attn_coverage)\n",
    "\n",
    "            if attns[\"std\"] == None:\n",
    "                attns[\"std\"] = decoder_attn_std\n",
    "                attns[\"coverage\"] = decoder_attn_coverage\n",
    "            else:\n",
    "                attns[\"std\"].append(decoder_attn_std)\n",
    "                attns[\"coverage\"].append(decoder_attn_coverage)\n",
    "            tem = decoder_output.view(1, batch_size, -1) # 1, batch, output\n",
    "            tem= F.log_softmax(tem, -1) # 1, batch, output\n",
    "            decoder_outputs[i] = tem\n",
    "            current_tgt = tem.max(2)[1].view(1, batch_size) # 1, batch\n",
    "        \n",
    "        return decoder_outputs, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(DATA_SIZE, HIDDEN_SIZE, ENC_DROPOUT)\n",
    "attention = Attention(HIDDEN_SIZE)\n",
    "decoder = Decoder(HIDDEN_SIZE, DATA_SIZE, attention, DEC_DROPOUT)\n",
    "\n",
    "model = S2S(encoder, decoder, DATA_SIZE, GENERATION_LEN).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# generator parameters:', sum(param.numel() for param in model.parameters() if param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = lang.word2index['<pad>']\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_IDX, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, criterion, clip, teach):    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for _, batch in enumerate(dataset):\n",
    "        covloss = 0\n",
    "        theme = batch[0].permute(1, 0) # (batch,1)\n",
    "        keyword = batch[1].permute(1, 0) # (batch,1)\n",
    "        src = batch[2].permute(1, 0) # (batch, s_len)\n",
    "        tgt = batch[3].permute(1, 0) # (batch, t_len)\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        outputs, attns = model(theme, keyword, src, tgt, teach)\n",
    "        \n",
    "        cov = attns.get(\"coverage\", None)\n",
    "        std = attns.get(\"std\", None)\n",
    "        for ii in range(len(cov)):\n",
    "            covloss += torch.min(std[ii], cov[ii]).sum()            \n",
    "        covloss *= LAMBDA_COVERAGE\n",
    "        outputs = outputs.contiguous()\n",
    "        outputs = outputs.view(-1, DATA_SIZE)\n",
    "        tgt = tgt.contiguous()\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss = loss + covloss\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=8, threshold=0.0001, cooldown=1, eps=1e-9)\n",
    "plot_loss = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    print(optimizer.param_groups[-1]['lr'])\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP, TEACH_RATE)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.2f}')\n",
    "    \n",
    "    plot_loss.append(train_loss)      \n",
    "    \n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    if epoch % CHECKPOINT_TIMES == 0:\n",
    "        model_name = f\"...\"\n",
    "        para_name = f\"...\"\n",
    "        torch.save(model, model_name) \n",
    "        checkpoint = {\"net\": model.state_dict(),\n",
    "                      \"optimizer\": optimizer.state_dict(),\n",
    "                      \"epoch\": epoch+1,\n",
    "                      \"scheduler\": scheduler.state_dict(),\n",
    "                      \"loss\": plot_loss}#\"scheduler\": scheduler.state_dict()\n",
    "        torch.save(checkpoint, para_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start from checkpoint\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=8, threshold=0.0001, cooldown=1, eps=1e-9)\n",
    "\n",
    "path_checkpoint = \"...\"  # checkepoint path\n",
    "checkpoint = torch.load(path_checkpoint)  # load checkpoint\n",
    "model.load_state_dict(checkpoint['net'])  # load parameters\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])  # load optimizer\n",
    "start_epoch = checkpoint['epoch']  # set start epoch\n",
    "scheduler.load_state_dict(checkpoint['scheduler']) # load scheduler\n",
    "plot_loss = checkpoint['loss'] # load loss\n",
    "\n",
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "    print(optimizer.param_groups[-1]['lr'])\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, CLIP, TEACH_RATE)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.2f}')\n",
    "    \n",
    "    plot_loss.append(train_loss)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    if epoch % CHECKPOINT_TIMES == 0:\n",
    "        model_name = f\"...\"\n",
    "        para_name = f\"...\"\n",
    "        torch.save(model, model_name) \n",
    "        checkpoint = {\n",
    "                      \"net\": model.state_dict(),\n",
    "                      \"optimizer\": optimizer.state_dict(),\n",
    "                      \"epoch\": epoch+1,\n",
    "                      \"scheduler\": scheduler.state_dict(),\n",
    "                      \"loss\": plot_loss}#\"scheduler\": lr_schedule.state_dict()\n",
    "        torch.save(checkpoint, para_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# test load\n",
    "model = torch.load(\"\")\n",
    "model.eval()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = get_datas(TEST_DATA_PATH)\n",
    "test_dataset = Create_Dataset(test_datas)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    teach = 0\n",
    "    result = []\n",
    "    \n",
    "    for _, batch in enumerate(dataset):\n",
    "        theme = batch[0].permute(1, 0) # (batch,1)\n",
    "        keyword = batch[1].permute(1, 0) # (batch,1)\n",
    "        src = batch[2].permute(1, 0) # (batch, s_len)\n",
    "        tgt = batch[3].permute(1, 0) # (batch, t_len)\n",
    "        outputs, attns = model(theme, keyword, src, tgt, teach)        \n",
    "        outputs = outputs.contiguous()\n",
    "        res = outputs.max(2)[1]\n",
    "        \n",
    "        tem = [[] for i in range(TEST_BATCH_SIZE)]\n",
    "        for i in test_res:\n",
    "            for j in range(len(i)):\n",
    "                tt = lang.index2word[i[j].cpu().numpy().tolist()]\n",
    "                tem[j].append(tt)\n",
    "        ans = []\n",
    "        for word in tem:\n",
    "            sent = \"\"\n",
    "            for itera in word:\n",
    "                sent += itera\n",
    "                sent += ' '\n",
    "                if itera == '<eos>':\n",
    "                    break\n",
    "            ans.append(sent)\n",
    "        result.append(ans)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = eval(model, test_dataloader)\n",
    "for batch in ans:\n",
    "    for sentence in batch:\n",
    "        print(sentence)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"res = [[] for i in range(TEST_BATCH_SIZE)]\n",
    "for i in test_res:\n",
    "    for j in range(len(i)):\n",
    "        tt = lang.index2word[i[j].cpu().numpy().tolist()]\n",
    "        res[j].append(tt)\n",
    "ans = []\n",
    "for word in res:\n",
    "    sent = \"\"\n",
    "    for itera in word:\n",
    "        sent += itera\n",
    "        sent += ' '\n",
    "        if itera == '<eos>':\n",
    "            break\n",
    "    ans.append(sent)\n",
    "for sentence in ans:\n",
    "    print(sentence)\n",
    "    print('\\n')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
